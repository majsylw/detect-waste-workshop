{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majsylw/detect-waste-workshop/blob/main/Detecting_trash_in_a_wild_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "Author: Sylwia Majchrowska\n",
        "\n",
        "\n",
        "<h1>Welcome to the workshop notebook \"Detecting trash in a wild\"!</h1>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/graphic.jpg\" alt=\"logo\" width=\"700\"/>\n",
        "\n",
        "This workshop gives an overview of AI applications in waste detection and introduces a pipeline for waste classification and detection based on the TACO dataset. It explores strategies for improving model accuracy, such as data augmentation and various learning approaches, and showcases successful waste management projects in diverse settings."
      ],
      "metadata": {
        "id": "oPhd1YwBCy5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook handling"
      ],
      "metadata": {
        "id": "ry6i0VWBmo_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Access\n",
        "\n",
        "To avoid installing all libraries and dependencies, you can use [Google Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb) - a Python workspace in the cloud. To do this, you need to move all data (as well as this notebook) to your Google Drive.\n",
        "\n",
        "You can access files on Google Drive by connecting (mapping) Google Drive in the virtual machine of the execution environment (notebook). To do this, execute the two code cells below.\n",
        "\n",
        "**NOTE:** Before executing the script below, make sure that you have uploaded the necessary data to your Google Drive and edited the access paths.\n",
        "\n",
        "**NOTE:** If you prefer to work on your own device, you need to install (or make sure you have installed) a [Python interpreter](https://docs.anaconda.com/anaconda/install/windows/) and the modules used in this notebook - you can find them by looking at all the instructions with the keyword *import*."
      ],
      "metadata": {
        "id": "KnuT_-Pebj1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CFFGE0WDnBcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the TACO repo\n",
        "!git clone https://github.com/pedropro/TACO\n",
        "\n",
        "# Move the folder that contains the JSON annotations to pwd\n",
        "#!cp -r TACO/data .\n",
        "# Download the images\n",
        "#!python TACO/download.py\n",
        "\n",
        "# For simplicity we will use provided data at google drive\n",
        "HOME_FOLDER = '/content/drive/MyDrive/detect-waste-workshop/'\n",
        "TACO_DATA_FOLDER = HOME_FOLDER + 'TACO/'\n",
        "MODELS_FOLDER = HOME_FOLDER + 'models/'\n",
        "CLS_FOLDER = HOME_FOLDER + 'images_square/'"
      ],
      "metadata": {
        "id": "I4w3Bue0nNiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python libraries\n",
        "\n",
        "**Useful imports** (select the below cell and press shift-enter to execute it)"
      ],
      "metadata": {
        "id": "6jY48QRYnB1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import matplotlib.pyplot as plt  # Graphic library for drawing charts\n",
        "# Special command for Jupyter notebooks to display images between cells, not in a new window\n",
        "%matplotlib inline\n",
        "\n",
        "import torch                                            # PyTorch - deep learning library\n",
        "from torchvision import datasets, models, transforms    # PyTorch extension for data management\n",
        "import torch.nn as nn                                   # Special PyTorch module for managing neural networks\n",
        "from torch.nn import functional as F                    # Special functions\n",
        "import torch.optim as optim                             # A set of algorithms that updates weights\n",
        "\n",
        "import requests\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "import torchvision.transforms as T\n",
        "torch.set_grad_enabled(False);\n",
        "\n",
        "import sklearn\n"
      ],
      "metadata": {
        "id": "LpI5rGsZnB_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper or Plastics? Classifying Objects!\n",
        "\n",
        "We will create a neural network here, train it, and then check how well it handles completely new data for it. In order to define a neural network, we need to give it a well-defined task - the task will be the classification of images, more specifically, we will be recognizing the type of waste present in the pictures.\n",
        "\n",
        "## Detect-waste categories\n",
        "\n",
        "The [Detect-waste](https://detectwaste.netlify.app/) dataset is inspired by the waste segregation principles in Gdańsk, Poland, and it categorizes waste according to these principles, aiming to align with Polish recycling standards. The dataset proposes seven well-defined categories for sorting litter, which are reflective of the broader recycling categories recognized in Poland. These categories are:\n",
        "\n",
        "- **Bio**: food waste such as fruit, vegetables, herbs;\n",
        "- **Glass**: glass objects such as glass bottles, jars, or broken glass;\n",
        "- **Metal and Plastic**: metal and plastic rubbish such as beverage cans, beverage bottles, plastic shards, plastic food packaging, or plastic straws;\n",
        "- **Non-recyclable**: non-recyclable rubbish such as disposable diapers, pieces of string, polystyrene packaging, polystyrene elements, blankets, clothing, or used paper cups;\n",
        "- **Other**: construction and demolition, large-size waste (e.g. tires), used electronics and household appliances, batteries, paint and varnish cans, or expired medicines;\n",
        "- **Paper**: paper items such as receipts, food packaging, newspapers, or cartons;\n",
        "- **Unknown**: hard to recognize, obscured objects.\n",
        "\n",
        "These categories are based on the recycling rules in Gdańsk, Poland, and aim to provide a comprehensive framework for waste classification that can be used for automatic waste detection and sorting.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/dw-cat.jpg\" alt=\"DW-cat\"/>"
      ],
      "metadata": {
        "id": "Cq9s2x682Qkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Steps with PyTorch - Building a Neural Network\n",
        "\n",
        "Generally, a neural network looks like the image below. The input to the network is on the left side, and the output is on the right. The number of output neurons corresponds to the number of classes.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/what_is_nn_slide.jpg\" alt=\"net\"/>\n",
        "\n",
        "So let's define a similar architecture for our classifier:"
      ],
      "metadata": {
        "id": "bYnHcG4r3Mwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extends PyTorch's neural network baseclass\n",
        "class MyNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A very basic neural network.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(3, 224, 224)):\n",
        "        \"\"\"\n",
        "        Constructs a neural network.\n",
        "\n",
        "        input_dim: a tuple that represents \"channel x height x width\" dimensions of the input\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # the total number of RGB pixels in an image is the tensor's volume\n",
        "        num_in_features = input_dim[0] * input_dim[1] * input_dim[2]\n",
        "        # input layer\n",
        "        self.layer_0 = nn.Linear(num_in_features, 128)\n",
        "        # hidden layers\n",
        "        self.layer_1 = nn.Linear(128, 64)\n",
        "        self.layer_2= nn.Linear(64, 32)\n",
        "        # output layer, output size of 2\n",
        "        self.layer_3= nn.Linear(32, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Define the forward pass through our network.\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        # convert our RGB tensor into one long vector\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        # pass through our layers\n",
        "        x = F.relu(self.layer_0(x))\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        x = F.relu(self.layer_3(x))\n",
        "\n",
        "        # convert the raw output to probability predictions\n",
        "        x = F.softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "K1niOFnQ25Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # cuda:0 means the first cuda device found\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MyNet().to(device)  # load our simple neural network\n",
        "model"
      ],
      "metadata": {
        "id": "1urZL5Ue4Tcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essentially, our network looks like this:\n",
        "\n",
        "![simple-network](https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/architecture.jpg)"
      ],
      "metadata": {
        "id": "dO1IGFXI4eeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data and Data Loading\n",
        "\n",
        "We should make two separate subsets to train and test our model. That way, we know our model learns more than rote memorization."
      ],
      "metadata": {
        "id": "JoSjcCFm7Leq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Data inspection\n",
        "Let's look in our data folder to see what's there."
      ],
      "metadata": {
        "id": "w26yPMHgAGew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # interact with the os. in our case, we want to view the file system\n",
        "\n",
        "class_dict = {}\n",
        "\n",
        "print(\"Data contents:\", os.listdir(CLS_FOLDER))\n",
        "for cl in os.listdir(CLS_FOLDER):\n",
        "  class_dict[cl] = len(os.listdir(CLS_FOLDER + cl))\n",
        "  print(f\"{cl} contents: {len(os.listdir(CLS_FOLDER + cl))} images\")"
      ],
      "metadata": {
        "id": "8s6si6ZT-xg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also look at some of the images\n",
        "\n",
        "from PIL import Image  # import our image opening tool\n",
        "\n",
        "_, ax = plt.subplots(2, 4, figsize=(15,15))  # to show 4 images side by side, make a \"1 row x 4 column\" axes\n",
        "ax[0, 0].set_title(\"metals_and_plastics\")\n",
        "ax[0, 0].imshow(Image.open(CLS_FOLDER + \"/metals_and_plastics/34.jpg\"))  # show the metals_and_plastics in the first column\n",
        "\n",
        "ax[0, 1].set_title(\"other\")\n",
        "ax[0, 1].imshow(Image.open(CLS_FOLDER + \"/other/206207.jpg\"))            # show the other in the second column\n",
        "\n",
        "ax[0, 2].set_title(\"non-recyclable\")\n",
        "ax[0, 2].imshow(Image.open(CLS_FOLDER + \"/non-recyclable/12.jpg\"))       # show the non-recyclable in the third column\n",
        "\n",
        "ax[0, 3].set_title(\"bio\")\n",
        "ax[0, 3].imshow(Image.open(CLS_FOLDER + \"/bio/162163.jpg\"))              # show the muffin in the fourth column\n",
        "\n",
        "ax[1, 0].set_title(\"glass\")\n",
        "ax[1, 0].imshow(Image.open(CLS_FOLDER + \"/glass/01.jpg\"))                # show the glass in the first column\n",
        "\n",
        "ax[1, 1].set_title(\"paper\")\n",
        "ax[1, 1].imshow(Image.open(CLS_FOLDER + \"/paper/23.jpg\"))                # show the paper in the second column\n",
        "\n",
        "ax[1, 2].set_title(\"unknown\")\n",
        "ax[1, 2].imshow(Image.open(CLS_FOLDER + \"/unknown/204205.jpg\"))          # show the unknown in the third column\n",
        "\n",
        "ax[1, 3].set_axis_off()\n"
      ],
      "metadata": {
        "id": "DRsw7YXV7Mhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset is highly imbalanced, for the puprose of this workshop let's create 2 classes: `metals_and_plastics` and `non_metals_and_plastics`"
      ],
      "metadata": {
        "id": "B3VA1j7Ai_He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "CLS_FOLDER2 = HOME_FOLDER + 'images_square2/'\n",
        "\n",
        "if not os.path.exists(CLS_FOLDER2):\n",
        "  os.mkdir(CLS_FOLDER2)\n",
        "if not os.path.exists(os.path.join(CLS_FOLDER2, 'metals_and_plastics')):\n",
        "  os.mkdir(os.path.join(CLS_FOLDER2, 'metals_and_plastics'))\n",
        "if not os.path.exists(os.path.join(CLS_FOLDER2, 'non_metals_and_plastics')):\n",
        "  os.mkdir(os.path.join(CLS_FOLDER2, 'non_metals_and_plastics'))\n",
        "\n",
        "class_dict = {}\n",
        "class_dict['non_metals_and_plastics'] = 0\n",
        "\n",
        "print(\"Data contents:\", os.listdir(CLS_FOLDER))\n",
        "for cl in os.listdir(CLS_FOLDER):\n",
        "  if cl == 'metals_and_plastics':\n",
        "    class_dict[cl] = len(os.listdir(CLS_FOLDER + cl))\n",
        "    for f in (os.listdir(CLS_FOLDER + cl)):\n",
        "      shutil.copyfile(os.path.join(CLS_FOLDER, 'metals_and_plastics', f),\n",
        "                      os.path.join(CLS_FOLDER2, 'metals_and_plastics', f))\n",
        "  else:\n",
        "    class_dict['non_metals_and_plastics'] += len(os.listdir(CLS_FOLDER + cl))\n",
        "    for f in (os.listdir(CLS_FOLDER + cl)):\n",
        "      shutil.copyfile(os.path.join(CLS_FOLDER, cl, f),\n",
        "                      os.path.join(CLS_FOLDER2, 'non_metals_and_plastics', f))\n",
        "\n",
        "print(f\"metals_and_plastics contents: {class_dict['metals_and_plastics']} images\")\n",
        "print(f\"non_metals_and_plastics contents: {class_dict['non_metals_and_plastics']} images\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dOynPsuYi-cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load our data\n",
        "That's great that we have data! But we have to load all the images and convert them into a form that our neural network understands. Specifically, PyTorch works with Tensor objects. (A tensor is just a multidimensional matrix, i.e. an N-d array.)\n",
        "\n",
        "![image-tensors](https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/image_to_tensor.jpg)\n",
        "\n",
        "To easily convert our image data into tensors, we use the help of a \"dataloader.\" The dataloader packages data into convenient boxes for our model to use. You can think of it like one person passing boxes (tensors) to another.\n",
        "\n",
        "![dataloader](https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/dataloader_box_analogy.jpg)\n",
        "\n",
        "First, we define some \"transforms\" to convert images to tensors. We must do so for both our train and validation datasets.\n",
        "\n",
        "For more information about transforms, check out the link here: https://pytorch.org/docs/stable/torchvision/transforms.html"
      ],
      "metadata": {
        "id": "ggsw9A24_8_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 224\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# transforms for our training data\n",
        "train_transforms = transforms.Compose([\n",
        "    # resize to resnet input size\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    # transform image to PyTorch tensor object\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# these validation transforms are exactly the same as our train transforms\n",
        "validation_transforms = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "print(\"Train transforms:\", train_transforms)"
      ],
      "metadata": {
        "id": "6FjxO_qu_vI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we create the datasets, by passing the transforms to the ImageFolder constructor.\n",
        "\n",
        "Our classification problem exhibits a large imbalance in the distribution of the target classes: for instance there is several times more non-recyclable than bio samples. In such cases it is recommended to use stratified sampling to ensure that relative class frequencies is approximately preserved in each train and validation fold."
      ],
      "metadata": {
        "id": "_uXYireZAiXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.model_selection\n",
        "\n",
        "img_dataset = datasets.ImageFolder(root=CLS_FOLDER,\n",
        "                                   transform=train_transforms)\n",
        "\n",
        "validation_dataset = datasets.ImageFolder(root=CLS_FOLDER,\n",
        "                                          transform=validation_transforms)\n",
        "\n",
        "print(class_dict)\n",
        "class_array = []\n",
        "for i, k in enumerate(class_dict):\n",
        "  class_array += class_dict[k] * [i]\n",
        "\n",
        "train_idx, test_idx = sklearn.model_selection.train_test_split(\n",
        "    np.arange(len(class_array)), test_size=0.2, shuffle=True, stratify=class_array)"
      ],
      "metadata": {
        "id": "MwN7amZeH-tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets = {\n",
        "    'train':\n",
        "        torch.utils.data.Subset(img_dataset, train_idx),\n",
        "    'validation':\n",
        "        torch.utils.data.Subset(validation_dataset, test_idx)}\n",
        "\n",
        "print(\"==Dataset==\\n\", image_datasets[\"train\"].dataset)"
      ],
      "metadata": {
        "id": "IK5beqGCTtT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, form dataloaders from the datasets:"
      ],
      "metadata": {
        "id": "d66svhZYWLlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\n",
        "    'train':\n",
        "        torch.utils.data.DataLoader(\n",
        "            image_datasets['train'],\n",
        "            batch_size=8,\n",
        "            shuffle=True,\n",
        "            num_workers=4),\n",
        "    'validation':\n",
        "        torch.utils.data.DataLoader(\n",
        "            image_datasets['validation'],\n",
        "            batch_size=8,\n",
        "            shuffle=False,\n",
        "            num_workers=4)}\n",
        "\n",
        "print(\"Train loader:\", dataloaders[\"train\"])\n",
        "print(\"Validation loader:\", dataloaders[\"validation\"])"
      ],
      "metadata": {
        "id": "X9pvZqMHWE2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see a dataloader outputs 2 things: a BIG tensor to represent an image, and a vector to represent the labels (from 0 to 6)."
      ],
      "metadata": {
        "id": "Ms7dlp-QWQTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(dataloaders[\"train\"]))"
      ],
      "metadata": {
        "id": "NkGo-t76WQdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model!\n",
        "Hurray! We've built a neural network and have data to give it. Now we repeatedly iterate over the data to train the model.\n",
        "\n",
        "Every time the network gets a new example, it looks something like this. Note the forward pass and the corresponding backward pass.\n",
        "\n",
        "![backpropagation](https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/backpropagation.gif)"
      ],
      "metadata": {
        "id": "S3wspSVWW3I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the train loop\n",
        "We want the network to learn from every example in our training dataset. However, the best performance comes from more practice. Therefore, we run through our dataset for multiple epochs.\n",
        "\n",
        "After each epoch, we'll check how our model performs on the validation set to monitor its progress."
      ],
      "metadata": {
        "id": "FWYeP6Z_XBRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tnrange, tqdm_notebook # import progress bars to show train progress\n",
        "\n",
        "def train_model(model, dataloaders, loss_function, optimizer, num_epochs):\n",
        "    \"\"\"\n",
        "    Trains a model using the given loss function and optimizer, for a certain number of epochs.\n",
        "\n",
        "    model: a PyTorch neural network\n",
        "    loss_function: a mathematical function that compares predictions and labels to return an error\n",
        "    num_epochs: the number of times to run through the full training dataset\n",
        "    \"\"\"\n",
        "    epoch_loss_list = []\n",
        "    epoch_acc_list = []\n",
        "\n",
        "    # train for n epochs. an epoch is a full iteration through our dataset\n",
        "    for epoch in tnrange(num_epochs, desc=\"Total progress\", unit=\"epoch\"):\n",
        "        # print a header\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('----------------')\n",
        "\n",
        "        # first train over the dataset and update weights; at the end, calculate our validation performance\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            # keep track of the overall loss and accuracy for this batch\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # iterate through the inputs and labels in our dataloader\n",
        "            # (the tqdm_notebook part is to display a progress bar)\n",
        "            for inputs, labels in tqdm_notebook(dataloaders[phase], desc=phase, unit=\"batch\", leave=False):\n",
        "                # move inputs and labels to appropriate device (GPU or CPU)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # FORWARD PASS\n",
        "                outputs = model(inputs)\n",
        "                # compute the error of the model's predictions\n",
        "                loss = loss_function(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    # BACKWARD PASS\n",
        "                    loss = torch.autograd.Variable(loss, requires_grad=True)\n",
        "                    optimizer.zero_grad()  # clear the previous gradients\n",
        "                    loss.backward()        # backpropagate the current error gradients\n",
        "                    optimizer.step()       # update the weights (i.e. do the learning)\n",
        "\n",
        "                # track our accumulated loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # track number of correct to compute accuracy\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # print our progress\n",
        "            epoch_loss = running_loss / len(image_datasets[phase])\n",
        "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
        "            print(f'{phase} error: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "            if phase == 'train':\n",
        "              epoch_loss_list.append(epoch_loss)\n",
        "            if phase == 'validation':\n",
        "              epoch_acc_list.append(epoch_acc.item())\n",
        "\n",
        "    return epoch_loss_list, epoch_acc_list"
      ],
      "metadata": {
        "id": "2jjQI3bVXBah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function and optimizer\n",
        "One last thing: we must define a function that gives feedback for how well the model performs. This is the loss, or \"error\" function, that compares model predictions to the true labels.\n",
        "\n",
        "Once we calculate the error, we also need to define how the model should react to that feedback. The optimizer determines how the network learns from feedback.\n",
        "\n",
        "![gradients](https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/gradient_descent.gif)"
      ],
      "metadata": {
        "id": "bYWcSK5wXK1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss()               # the most common error function in deep learning\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-6)  # Stochastic Gradient Descent, with a learning rate of 0.1"
      ],
      "metadata": {
        "id": "8M7SQepwXTjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run training\n",
        "Let's put everything together and TRAIN OUR MODEL! =D"
      ],
      "metadata": {
        "id": "EcJCIzXBXUsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = train_model(model, dataloaders, loss_function, optimizer, num_epochs=3)"
      ],
      "metadata": {
        "id": "ryJc9OdsXU-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16,5))\n",
        "ax[0].plot(loss)\n",
        "ax[0].set_title(\"Loss\")\n",
        "ax[1].plot(acc)\n",
        "ax[1].set_title(\"Accuracy\")"
      ],
      "metadata": {
        "id": "djQHqJ8reTk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Can You Do Better?\n",
        "Now that we've shown you how to train a neural network, can you improve the validation accuracy by tweaking the parameters?\n",
        "\n",
        "Some parameters to play with:\n",
        "\n",
        "- Number of epochs\n",
        "- The learning rate \"lr\" parameter in the optimizer\n",
        "- The type of optimizer (https://pytorch.org/docs/stable/optim.html)\n",
        "- Type of neural network (Number of layers and layer dimensions)\n",
        "- Image size\n",
        "- Data augmentation transforms (https://pytorch.org/vision/stable/transforms.html)"
      ],
      "metadata": {
        "id": "IYtEYiekn5EA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transfer Learning using EfficientNet PyTorch\n",
        "\n",
        "Now, we will carry out the transfer learning training. I propose to use EfficientNet=B0 model, which is a part of the EfficientNet models family. Models available in `torchvision` package have already been trained on the ImageNet dataset. The power of these pretrained models actually shines when we have a small dataset to train on. In such situations, training from scratch does not really help much. Transfer learning addresses these challenges effectively by leveraging knowledge gained from pre-trained models on large datasets. Here's how transfer learning proves useful for litter classification with the TACO dataset:\n",
        "\n",
        "**Leveraging Pre-trained Models**\n",
        "- Rapid Development: Starting with a model pre-trained on a large dataset (like ImageNet) allows for quicker convergence on the specific task of litter classification, reducing the time and computational resources required for training from scratch\n",
        "- Improved Performance: Pre-trained models have learned rich feature representations that can be effectively transferred to the litter classification task, often resulting in improved accuracy and robustness compared to models trained from scratch, especially when the available training data is limited\n",
        "\n",
        "**Addressing Data Scarcity and Diversity**\n",
        "- Data Scarcity: While the TACO dataset is comprehensive, it may not cover the wide variety of litter types and conditions found in real-world scenarios. Transfer learning allows models to leverage learned features from broader contexts, making them more adaptable to new or unseen litter types.\n",
        "- Variability and Complexity: Litter can vary greatly in appearance, and its detection is complicated by diverse backgrounds and conditions. Models pre-trained on large and varied datasets have encountered a wide range of objects and scenarios, equipping them with a better understanding of complex visual patterns\n",
        "\n",
        "**Enhancing Generalization**\n",
        "- Generalization to New Environments: The ability of transfer learning models to generalize from one task to another is particularly valuable for litter classification, where the model needs to perform well across different environments, such as urban areas, beaches, and natural landscapes\n",
        "- Robustness to Overfitting: By fine-tuning pre-trained models on the TACO dataset, the risk of overfitting is reduced, as the model has already learned generalizable features. This is especially important when working with a limited amount of labeled data\n",
        "\n",
        "**Facilitating Innovation**\n",
        "- Innovation in Model Architecture: Transfer learning encourages experimentation with different model architectures. Researchers can explore how various pre-trained models perform on the TACO dataset, leading to insights that drive further innovation in litter detection technology\n",
        "- Cross-Domain Applications: The success of transfer learning in litter classification can inspire similar approaches in related domains, such as recycling sorting or environmental monitoring, broadening the impact of machine learning solutions in sustainability efforts\n",
        "\n",
        "In conclusion, transfer learning is invaluable for enhancing the performance, efficiency, and applicability of machine learning models for litter classification based on the TACO dataset. It leverages the strengths of pre-trained models to address the unique challenges of litter detection, making it a cornerstone technique in the development of advanced waste management solutions."
      ],
      "metadata": {
        "id": "C8-ctqt1seI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_model(pretrained=True, fine_tune=True, num_classes=7):\n",
        "    if pretrained:\n",
        "        print('[INFO]: Loading pre-trained weights')\n",
        "    else:\n",
        "        print('[INFO]: Not loading pre-trained weights')\n",
        "    model = models.efficientnet_b0(pretrained=pretrained)\n",
        "    if fine_tune:\n",
        "        print('[INFO]: Fine-tuning all layers...')\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = True\n",
        "    elif not fine_tune:\n",
        "        print('[INFO]: Freezing hidden layers...')\n",
        "        for params in model.parameters():\n",
        "            params.requires_grad = False\n",
        "    # Change the final classification head.\n",
        "    model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes)\n",
        "    return model\n",
        "\n",
        "model = build_model(\n",
        "    pretrained=True,\n",
        "    fine_tune=True,\n",
        "    num_classes=len(class_dict)).to(device)\n",
        "\n",
        "# Total parameters and trainable parameters.\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")\n",
        "# Optimizer.\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
        "# Loss function.\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "loss, acc = train_model(model, dataloaders, loss_function, optimizer,\n",
        "                        num_epochs=3)"
      ],
      "metadata": {
        "id": "0DXfIfqCrWTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16,5))\n",
        "ax[0].plot(loss)\n",
        "ax[0].set_title(\"Loss\")\n",
        "ax[1].plot(acc)\n",
        "ax[1].set_title(\"Accuracy\")"
      ],
      "metadata": {
        "id": "Uxiz7PlOrWgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "NJdOrF4ZxYLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "\n",
        "model = timm.create_model('swin_tiny_patch4_window7_224',\n",
        "                          pretrained=True,\n",
        "                          num_classes=len(class_dict)).to(device)\n",
        "\n",
        "# Total parameters and trainable parameters.\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} training parameters.\")\n",
        "# Optimizer.\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
        "# Loss function.\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "loss, acc = train_model(model, dataloaders, loss_function, optimizer,\n",
        "                        num_epochs=5)"
      ],
      "metadata": {
        "id": "bIA3poiswkjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16,5))\n",
        "ax[0].plot(loss)\n",
        "ax[0].set_title(\"Loss\")\n",
        "ax[1].plot(acc)\n",
        "ax[1].set_title(\"Accuracy\")"
      ],
      "metadata": {
        "id": "Ced0vNbB43QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examine model performance\n",
        "\n",
        "How do we examine our model's predictions? Let's visualize predicted by model classes."
      ],
      "metadata": {
        "id": "n0u2aqA5Yym-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def move_to_gpu(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [move_to_gpu(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "def predict(image, model):\n",
        "    xb = move_to_gpu(image.unsqueeze(0), device)\n",
        "    yb = model(xb)\n",
        "    _, preds = torch.max(yb, dim=1)\n",
        "    return validation_dataset.classes[preds[0].item()]\n",
        "\n",
        "img, label = random.choice(validation_dataset)\n",
        "plt.imshow(img.permute(1, 2, 0))  # Permuting the image to the format expected by matplotlib\n",
        "plt.title(f'Actual Class: {validation_dataset.classes[label]}, Predicted Class: {predict(img, model)}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lCkrkT6i2yhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows you to visualize the performance of a model."
      ],
      "metadata": {
        "id": "Xa01LgNd0nRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, classes):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images, labels = batch\n",
        "            images = move_to_gpu(images, device)  # Move images to GPU if available\n",
        "            labels = labels.to(device)            # Move labels to GPU if available\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())    # Collect predictions\n",
        "            all_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(model, dataloaders[\"validation\"], validation_dataset.classes)"
      ],
      "metadata": {
        "id": "nUC0HDCryAdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, especially in classification tasks, evaluating the performance of a model is crucial. The scikit-learn library provides several metrics for this purpose, including `accuracy_score`, `precision_score`, `recall_score`, and `f1_score`. Here's a brief overview of each:\n",
        "\n",
        "`accuracy_score`\n",
        "- Definition: Accuracy is the simplest and most intuitive performance metric. It calculates the ratio of the number of correct predictions to the total number of predictions.\n",
        "- Formula: (Accuracy = $\\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$)\n",
        "- Use Case: While accuracy is straightforward, it may not be the best metric for imbalanced datasets, where the number of instances in each class varies significantly.\n",
        "\n",
        "`precision_score`\n",
        "- Definition: Precision measures the accuracy of positive predictions. It is the ratio of true positive predictions to the total number of positive predictions (including false positives).\n",
        "- Formula: (Precision = $\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$)\n",
        "- Use Case: Precision is crucial when the cost of a false positive is high. For example, in spam detection, a false positive (marking a legitimate email as spam) is usually seen as more problematic than a false negative (failing to detect a spam email).\n",
        "\n",
        "`recall_score`\n",
        "- Definition: Recall (also known as sensitivity) measures the ability of a model to find all the relevant cases within a dataset. It is the ratio of true positive predictions to the total number of actual positives.\n",
        "- Formula: (Recall = $\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$)\n",
        "- Use Case: Recall is important when the cost of a false negative is high. For instance, in medical diagnosis, failing to detect a disease (false negative) could be more critical than incorrectly diagnosing a disease (false positive).\n",
        "\n",
        "`f1_score`\n",
        "- Definition: The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is particularly useful when you need to balance precision and recall.\n",
        "- Formula: (F1 = $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$)\n",
        "- Use Case: The F1 score is valuable in situations where neither false positives nor false negatives can be favored over the other, such as in document classification or customer support ticket categorization.\n",
        "\n",
        "Each of these metrics offers a different perspective on the performance of a classification model, and the choice of which metric(s) to use depends on the specific requirements and constraints of the task at hand."
      ],
      "metadata": {
        "id": "XUx6CnrazOaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def export_classification_metrics(model, dataloader, classes):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images, labels = batch\n",
        "            images = move_to_gpu(images, device)  # Move images to GPU if available\n",
        "            labels = labels.to(device)  # Move labels to GPU if available\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())  # Collect predictions\n",
        "            all_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
        "\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_labels = [1 if label == i else 0 for label in all_labels]\n",
        "        class_preds = [1 if pred == i else 0 for pred in all_preds]\n",
        "\n",
        "        accuracy = accuracy_score(class_labels, class_preds) * 100\n",
        "        precision = precision_score(class_labels, class_preds, zero_division=0) * 100\n",
        "        recall = recall_score(class_labels, class_preds, zero_division=0) * 100\n",
        "        f1 = f1_score(class_labels, class_preds, zero_division=0) * 100\n",
        "\n",
        "        print(f'{class_name}, accuracy: {accuracy:.2f}%, precision: {precision:.2f}%, recall: {recall:.2f}%, F1 score: {f1:.2f}%')\n",
        "\n",
        "export_classification_metrics(model, dataloaders[\"validation\"], validation_dataset.classes)"
      ],
      "metadata": {
        "id": "rNOGe4C5yIIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional excercise - Peekaboo, I Found You! Localization and classification in one.\n",
        "\n",
        "Detection, i.e., simultaneous localization and classification of objects, is a much more challenging task. For this reason, in this case, we will not create and train our own network, but we will use an already available model provided by the [Facebook team](https://github.com/facebookresearch/detr)."
      ],
      "metadata": {
        "id": "TbquLVTkaRNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import requests\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "import torchvision.transforms as T\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import pylab\n",
        "from shutil import copyfile\n",
        "import os, sys\n",
        "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
      ],
      "metadata": {
        "id": "dnnnBUP3Cnfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classes in COCO\n",
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# colors for visualization purposes\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
      ],
      "metadata": {
        "id": "DxIvXAG0aZcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization for COCO data\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# functions to convert bounding box positions for the found objects\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b"
      ],
      "metadata": {
        "id": "HxLqXjM7aa0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(pil_img, prob, boxes):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=3))\n",
        "        cl = p.argmax()\n",
        "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
        "        ax.text(xmin, ymin, text, fontsize=15,\n",
        "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Kwu0gwLNawAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will download the pre-trained detector model from the Internet.\n",
        "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "UwY07cOHa4zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading an image from a specific website\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "im = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# loading an image from disk\n",
        "# im = Image.open('./images/000000039769.jpg')"
      ],
      "metadata": {
        "id": "_747EZCKa6um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image normalization\n",
        "img = transform(im).unsqueeze(0)\n",
        "\n",
        "# passing through the network\n",
        "outputs = model(img)\n",
        "\n",
        "# keeping only the predictions for which the network was at least 90% confident\n",
        "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "keep = probas.max(-1).values >= 0.9\n",
        "\n",
        "# calculating the object locations in pixels\n",
        "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)"
      ],
      "metadata": {
        "id": "Do8MyU0kbIJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(im, probas[keep], bboxes_scaled)"
      ],
      "metadata": {
        "id": "rc04nPujbTjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning DETR on TACO"
      ],
      "metadata": {
        "id": "jPJjGAedExnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/wimlds-trojmiasto/detect-waste.git\n",
        "\n",
        "# add modules from detr path\n",
        "sys.path.append('detect-waste/detr/')"
      ],
      "metadata": {
        "id": "Swt45pWEF23Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up path to annotations\n",
        "annFile = TACO_DATA_FOLDER + 'annotations.json'\n",
        "\n",
        "# initialize COCO api for instance annotations\n",
        "coco = COCO(annFile)\n",
        "\n",
        "# display COCO categories and supercategories\n",
        "cats = coco.loadCats(coco.getCatIds())\n",
        "nms=[cat['name'] for cat in cats]\n",
        "print('COCO categories: \\n{}\\n'.format(', '.join(nms)))"
      ],
      "metadata": {
        "id": "m5usT3zdbVwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and display image\n",
        "catIds = coco.getCatIds(catNms=['non recyclable'])\n",
        "imgIds = coco.getImgIds(catIds=catIds)\n",
        "img_id = imgIds[np.random.randint(0,len(imgIds))]\n",
        "print('Image n°{}'.format(img_id))\n",
        "\n",
        "img = coco.loadImgs(img_id)[0]\n",
        "\n",
        "img_name = '%s/%s'%(TACO_DATA_FOLDER, img['file_name'])\n",
        "print('Image name: {}'.format(img_name))\n",
        "\n",
        "I = io.imread(img_name)\n",
        "plt.figure()\n",
        "plt.imshow(I)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "AWtjzu73baee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and display instance annotations\n",
        "plt.imshow(I); plt.axis('off')\n",
        "annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds)\n",
        "anns = coco.loadAnns(annIds)\n",
        "coco.showAnns(anns, draw_bbox=True)"
      ],
      "metadata": {
        "id": "PDZuiFSkFo-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUTS = HOME_FOLDER + \"outputs/\"\n",
        "\n",
        "!python detect-waste/detr/main.py \\\n",
        "  --dataset_file \"coco\" \\\n",
        "  --coco_path TACO_DATA_FOLDER \\\n",
        "  --output_dir OUTPUTS \\\n",
        "  --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth \\\n",
        "  --epochs 10 \\\n",
        "  --num_classes 7"
      ],
      "metadata": {
        "id": "c59pCSZbFvdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}