{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majsylw/detect-waste-workshop/blob/main/Detecting_trash_in_a_wild_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPhd1YwBCy5h"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "Author: Sylwia Majchrowska\n",
        "\n",
        "\n",
        "<h1>Welcome to the workshop notebook \"Exploring Trash Annotations in Context\"!</h1>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/graphic.jpg\" alt=\"logo\" width=\"700\"/>\n",
        "\n",
        "This workshop provides an overview of waste types using public dataset, covering data cleaning, preparation, and labeling standards. Participants will also conduct exploratory data analysis to understand data and label quality and their impact on machine learning training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbzrkP9nEbpx"
      },
      "source": [
        "## Notebook Handling\n",
        "The file you are reading is a [Jupyter Notebook](https://jupyter.org/). It is not a static page but an interactive environment that allows you to create and execute code written in the Python language.\n",
        "\n",
        "The notebook consists of two types of cells: for entering text and for source code. In one cell, you can enter multiple lines of code, but it is advisable to do so thoughtfully and in moderation, as all commands placed in one cell will execute sequentially when it is run (Shift+Enter). Below is an example of a cell with code that saves a certain value to a variable and prints its contents on the screen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCEp6gmjEKFh"
      },
      "outputs": [],
      "source": [
        "number_of_seconds_in_a_day = 24 * 60 * 60\n",
        "number_of_seconds_in_a_day"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XGp_1mbEr2R"
      },
      "source": [
        "To execute the code in the above cell, select it by clicking on it and then simultaneously press the \"Shift+Enter\" key combination (or click the play button, which appears to the left of the code after clicking the cell).\n",
        "\n",
        "Within one notebook, all code cells share a common memory. Furthermore, the order in which commands in the cells are run depends on you. This means that if you create an object in memory and give it a name in one cell, every subsequently executed cell will be aware of this object. This has its advantages and disadvantages. An important side effect of this solution is that the events in the notebook depend strictly on the order in which its cells are run. Below is an example prepared to illustrate the described property of notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cvYaO9vEs1X"
      },
      "outputs": [],
      "source": [
        "number_of_seconds_in_a_week = 7 * number_of_seconds_in_a_day\n",
        "number_of_seconds_in_a_week"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEjEYgmNE1dw"
      },
      "source": [
        "#### Data Access\n",
        "\n",
        "To avoid installing all libraries and dependencies, you can use [Google Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb) - a Python workspace in the cloud. To do this, you need to move all data (as well as this notebook) to your Google Drive.\n",
        "\n",
        "You can access files on Google Drive by connecting (mapping) Google Drive in the virtual machine of the execution environment (notebook). To do this, execute the two code cells below.\n",
        "\n",
        "**NOTE:** Before executing the script below, make sure that you have uploaded the necessary data to your Google Drive and edited the access paths.\n",
        "\n",
        "**NOTE:** If you prefer to work on your own device, you need to install (or make sure you have installed) a [Python interpreter](https://docs.anaconda.com/anaconda/install/windows/) and the modules used in this notebook - you can find them by looking at all the instructions with the keyword *import*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy-4iOj0FS1w",
        "outputId": "e1fd1296-30a0-42b9-be31-9dea2c5da228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJbHHhppERmM"
      },
      "source": [
        "### Python libraries\n",
        "\n",
        "**Usefull imports** (select the below cell and press shift-enter to execute it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfBM_z3kERw3"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import pylab\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from graphviz import Digraph  # Note: graphviz may require more than pip installation due to path issue: e.g. brew install graphviz for mac\n",
        "\n",
        "from PIL import Image, ExifTags\n",
        "from pycocotools.coco import COCO\n",
        "from matplotlib.patches import Polygon, Rectangle\n",
        "from matplotlib.collections import PatchCollection\n",
        "import colorsys\n",
        "import random\n",
        "import pylab\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y405fgyIFWVR"
      },
      "source": [
        "# How Do Machines Learn?\n",
        "\n",
        "Some artificial intelligence technologies have been around for a long time, but advances in computational power, the availability of vast amounts of data, and new algorithms have led to major breakthroughs in this field. Artificial intelligence is widely used to provide personalized recommendations during shopping or ordinary web searches. More advanced inventions include autonomous - self-driving - cars, which, in a simplified manner, make decisions about their next moves based on data collected from various sensors installed in them.\n",
        "\n",
        "## Types of Machine Learning\n",
        "\n",
        "We can perceive machine learning as the art of extracting knowledge from data. The basic division of the field into sub-areas results from the type of task the machine is to solve:\n",
        "- Supervised learning: occurs when all data presented to the machine is labeled, i.e., marked, in exactly the same way as the expected response.\n",
        "- Unsupervised learning: occurs when we have a large amount of unlabeled data, and the machine's task is to determine the data structure, such as grouping them accordingly.\n",
        "- Reinforcement learning: through trial and error, the machine seeks a solution to a formulated task, being rewarded (when it acts correctly) or punished (when it makes mistakes), but otherwise, it is not given any hints or suggestions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAD7kYHLFrqZ"
      },
      "source": [
        "## Common Challenges\n",
        "\n",
        "As you've probably noticed, machine learning heavily relies on data. Therefore, both the quantity and quality of data are particularly important here. Just imagine that you are building an autonomous car - in simple terms, you need to teach the machine to recognize the road, the roadside, other vehicles, and pedestrians. If you only have data collected during the day, you won't be able to teach your device to react at night when the lighting of encountered objects is completely different. Furthermore, even with too few instances of daytime road images, you cannot be sure that you will train a network that will be able to react correctly in every situation - how would it know how to behave if it hadn't 'seen' a similar case before? We can use an extreme case here: a pedestrian wearing a shirt with black and white stripes. It would be unacceptable to release an algorithm that could mistake them for a pedestrian crossing, right? Moving on - let's assume you want to deploy your invention on the streets of New York, but during model training, you only had photos from Skopje. Both cities have different infrastructures, which also affects the device's operational efficiency.\n",
        "\n",
        "## Images - A Data Treasury for Computer Vision\n",
        "The example of machine learning discussed above relies largely on images. Computer vision, a field of artificial intelligence, aims to mimic machines' ability to understand what they see. In computer vision, a model is created based on images and the expected results (predictions) when dealing with supervised learning.\n",
        "\n",
        "**NOTE:** The accuracy of predictions depends on the quality of input data and the constructed model (a kind of *computer's eyes*).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/CV.jpg\" alt=\"cv\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFWRX7-fJnN4"
      },
      "source": [
        "# The Trash Annotations in Context (TACO) Dataset 🌮\n",
        "For the tutorial we will use the open-source [TACO dataset](http://tacodataset.org/). The dataset repository contains a official dataset with 1500 images and 4784 annotations and an unofficial dataset with 3736 images and 8419 annotations. Both datasets contain various large and small litter objects on different backgrounds such as streets, parks, and beaches. The dataset contains 60 categories. of litter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA-9G7-kKJ2H"
      },
      "source": [
        "## About the dataset\n",
        "\n",
        "- Research Paper: [TACO: Trash Annotations in Context for Litter Detection](https://arxiv.org/abs/2003.06975)\n",
        "- Author: Pedro F Proença, Pedro Simões\n",
        "- Dataset Size: Official: 1500 images, 4784 annotations & Unofficial: 3736 images, 8419 annotations.\n",
        "- Categories: 60 litter categories\n",
        "- License: CC BY 4.0\n",
        "- Release: 17 March 2020\n",
        "- Read more: [Github](https://github.com/pedropro/TACO) & [Webpage](http://tacodataset.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxRZ0CS5Kvx3"
      },
      "source": [
        "## Downloading the Dataset\n",
        "\n",
        "So, let’s download the dataset from the official website by following the instructions. Currently, there are two datasets, namely, TACO-Official and TACO-Unofficial.\n",
        "\n",
        "- The annotations of the official dataset are collected, annotated, and reviewed by the creators of the TACO project.\n",
        "- The annotations of the unofficial dataset are provided by the community and they are not reviewed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HR0qpbcFWpB"
      },
      "outputs": [],
      "source": [
        "# Clone the TACO repo\n",
        "!git clone https://github.com/pedropro/TACO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e39VjamfOdAs"
      },
      "outputs": [],
      "source": [
        "# Move the folder that contains the JSON annotations to pwd\n",
        "#!cp -r TACO/data .\n",
        "# Download the images\n",
        "#!python TACO/download.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqVpgVUAOe_c"
      },
      "outputs": [],
      "source": [
        "# For simplicity we will use provided data at google drive\n",
        "\n",
        "HOME_FOLDER = '/content/drive/MyDrive/detect-waste-workshop/'\n",
        "TACO_DATA_FOLDER = HOME_FOLDER + 'TACO/'\n",
        "MODELS_FOLDER = HOME_FOLDER + 'models/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFU8GtqaSFCl"
      },
      "source": [
        "## Understanding Data and Label Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JPC23uCCMER"
      },
      "source": [
        "The TACO dataset is a dataset specifically designed for the detection of litter in complex environments. It is built upon and extends the COCO (Common Objects in Context) format, which is a widely used standard for object detection, segmentation, and captioning tasks. The COCO format provides a structured approach to organizing image data and annotations, making it easier for machine learning models to learn from complex datasets.\n",
        "\n",
        "### COCO Annotations in the TACO Dataset\n",
        "In the context of the TACO dataset, [COCO annotations](https://cocodataset.org/#home) are used to describe various aspects of the images, particularly focusing on the litter items present. Here's a breakdown of how COCO annotations are typically structured and applied within the TACO dataset:\n",
        "\n",
        "- `Images`: Each entry contains information about the image, including a unique ID, file name, and dimensions (height and width).\n",
        "- `Categories`: This section defines the different types of litter or waste categories present in the dataset. Each category has a unique ID, a name (e.g., \"plastic bottle\", \"can\"), and sometimes a supercategory (e.g., \"plastic\" for \"plastic bottle\").\n",
        "- `Annotations`: Annotations link specific instances of objects (in this case, litter items) in the images to their categories. Each annotation includes:\n",
        "  - An ID for the annotation itself.\n",
        "  - The ID of the image the annotation belongs to.\n",
        "  - The category ID indicating the type of litter.\n",
        "  - A segmentation field that outlines the exact shape of the object within the image. This can be in the form of a polygon or a mask.\n",
        "  - A bounding box field that provides the coordinates of a rectangle enclosing the object.\n",
        "  - Additional attributes like area (of the segmentation or bounding box) and iscrowd (indicating if the object is part of a group or a single instance).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vIHAFmCDtin"
      },
      "outputs": [],
      "source": [
        "# Set up path to annotations\n",
        "anns_file_path = TACO_DATA_FOLDER + 'annotations.json'\n",
        "\n",
        "# Read annotations\n",
        "with open(anns_file_path, 'r') as f:\n",
        "    dataset = json.loads(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRKT6FWVEisy"
      },
      "outputs": [],
      "source": [
        "print(dataset.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hucNbmr2bfnU"
      },
      "outputs": [],
      "source": [
        "print(dataset['annotations'][0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVgiCRcSDhsK"
      },
      "source": [
        "### Purpose and Application\n",
        "The use of COCO annotations in the TACO dataset facilitates the application of advanced computer vision techniques for litter detection. By providing detailed annotations, the dataset allows for:\n",
        "\n",
        "- `Object Detection`: Identifying and locating litter items within images.\n",
        "  - `Annotation Format`: The annotations typically include bounding boxes that specify the coordinates of the rectangle enclosing each object. Along with the bounding box, each object is labeled with a category from a predefined set.\n",
        "  - `Example`: In a waste detection dataset, a plastic bottle on the beach might be annotated with a bounding box around the bottle and labeled as \"plastic.\"\n",
        "- `Segmentation`: Precisely outlining the shape of each litter item, useful for understanding the spatial distribution and context of litter in various environments.\n",
        "  - `Annotation Format`: Segmentation annotations are more detailed than detection annotations. They can be in the form of masks that outline the exact shape of each object, with each pixel in the mask being assigned a class label. For instance segmentation, each object instance is uniquely identified.\n",
        "  - `Example`: In the TACO dataset, a segmentation mask might precisely outline the shape of each piece of litter, differentiating between multiple pieces of the same type of waste.\n",
        "- `Classification`: Classifying litter items into predefined categories, aiding in waste management and recycling efforts.\n",
        "  - `Annotation Format`: These annotations are simpler, consisting of one or more labels that apply to the whole image or to identified objects without spatial information. In object-level classification, the objects are usually first detected or segmented.\n",
        "  - `Example`: An image of a landfill might be labeled with categories like \"organic waste,\" \"plastic,\" and \"metal,\" indicating the presence of these types of waste without specifying their locations or shapes.\n",
        "\n",
        "The structured format of COCO annotations, combined with the specific focus on litter in the TACO dataset, makes it a valuable resource for developing and testing machine learning models aimed at environmental conservation and waste management solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSUK5_bePPS1"
      },
      "source": [
        "#### Visualize Annotated Images\n",
        "\n",
        "For simplicity, to select and show the dataset images with the respective masks, we make use of the COCO API. The script below shows how to load and visualize an image with all its annotations.\n",
        "\n",
        "Unfortunately, several python libraries do not take into account the EXIF orientation tag, thus we have to explicitly rotate the images. Alternatively you can use instead OpenCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhHE5RLFPNr-"
      },
      "outputs": [],
      "source": [
        "# User settings\n",
        "image_filepath = 'batch_11/000028.jpg'\n",
        "pylab.rcParams['figure.figsize'] = (28,28)\n",
        "####################\n",
        "\n",
        "# Obtain Exif orientation tag code\n",
        "for orientation in ExifTags.TAGS.keys():\n",
        "    if ExifTags.TAGS[orientation] == 'Orientation':\n",
        "        break\n",
        "\n",
        "# Loads dataset as a coco object\n",
        "coco = COCO(anns_file_path)\n",
        "\n",
        "# Find image id\n",
        "img_id = -1\n",
        "for img in dataset['images']:\n",
        "    if img['file_name'] == image_filepath:\n",
        "        img_id = img['id']\n",
        "        break\n",
        "\n",
        "# Show image and corresponding annotations\n",
        "if img_id == -1:\n",
        "    print('Incorrect file name')\n",
        "else:\n",
        "    # Load image\n",
        "    print(image_filepath)\n",
        "    I = Image.open(TACO_DATA_FOLDER + image_filepath)\n",
        "\n",
        "    # Load and process image metadata\n",
        "    if I._getexif():\n",
        "        exif = dict(I._getexif().items())\n",
        "        # Rotate portrait and upside down images if necessary\n",
        "        if orientation in exif:\n",
        "            if exif[orientation] == 3:\n",
        "                I = I.rotate(180,expand=True)\n",
        "            if exif[orientation] == 6:\n",
        "                I = I.rotate(270,expand=True)\n",
        "            if exif[orientation] == 8:\n",
        "                I = I.rotate(90,expand=True)\n",
        "\n",
        "    # Show image\n",
        "    fig,ax = plt.subplots(1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(I)\n",
        "\n",
        "    # Load mask ids\n",
        "    annIds = coco.getAnnIds(imgIds=img_id, catIds=[], iscrowd=None)\n",
        "    anns_sel = coco.loadAnns(annIds)\n",
        "\n",
        "    # Show annotations\n",
        "    for ann in anns_sel:\n",
        "        color = colorsys.hsv_to_rgb(np.random.random(),1,1)\n",
        "        for seg in ann['segmentation']:\n",
        "            poly = Polygon(np.array(seg).reshape((int(len(seg)/2), 2)))\n",
        "            p = PatchCollection([poly], facecolor=color, edgecolors=color,linewidths=0, alpha=0.4)\n",
        "            ax.add_collection(p)\n",
        "            p = PatchCollection([poly], facecolor='none', edgecolors=color, linewidths=2)\n",
        "            ax.add_collection(p)\n",
        "        [x, y, w, h] = ann['bbox']\n",
        "        rect = Rectangle((x,y),w,h,linewidth=2,edgecolor=color,\n",
        "                         facecolor='none', alpha=0.7, linestyle = '--')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aZn0OumS670"
      },
      "outputs": [],
      "source": [
        "for ann in anns_sel:\n",
        "  for cat in dataset['categories']:\n",
        "    if cat['id'] == ann['category_id']:\n",
        "      print(cat['supercategory'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz7Cd5wSUC9c"
      },
      "source": [
        "#### Key Differences\n",
        "- Level of Detail: Segmentation annotations provide the highest level of detail, followed by detection annotations. Category classification annotations provide the least detail, focusing only on the presence of categories.\n",
        "- Use Cases: Detection is suitable for applications where the location of objects is important. Segmentation is used when precise outlines are needed, for example, in medical imaging or detailed environmental monitoring. Category classification is used for simpler tasks where only the presence of certain types of objects is relevant.\n",
        "- Complexity and Effort: Creating segmentation annotations requires the most effort due to the need for pixel-level precision. Detection annotations are less time-consuming but still require careful placement of bounding boxes. Category classification annotations are the simplest and fastest to create.\n",
        "\n",
        "Understanding these differences is crucial for selecting the appropriate annotation type for a given task, balancing the level of detail needed against the effort required to create the annotations.\n",
        "\n",
        "\n",
        "**NOTE:** Try yourself by annotating TACO images at http://tacodataset.org/annotate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1X816-bGAwd"
      },
      "source": [
        "### TACO waste type distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX59y_NkOh20"
      },
      "outputs": [],
      "source": [
        "# This shows the number of annotations per category\n",
        "\n",
        "categories = dataset['categories']\n",
        "anns = dataset['annotations']\n",
        "imgs = dataset['images']\n",
        "scenes = dataset['scene_categories']\n",
        "\n",
        "nr_cats = len(categories)\n",
        "nr_annotations = len(anns)\n",
        "nr_images = len(imgs)\n",
        "nr_scenes = len(scenes)\n",
        "\n",
        "# Load categories and super categories\n",
        "cat_names = []\n",
        "super_cat_names = []\n",
        "super_cat_ids = {}\n",
        "super_cat_last_name = ''\n",
        "nr_super_cats = 0\n",
        "for cat_it in categories:\n",
        "    cat_names.append(cat_it['name'])\n",
        "    super_cat_name = cat_it['supercategory']\n",
        "    # Adding new supercat\n",
        "    if super_cat_name != super_cat_last_name:\n",
        "        super_cat_names.append(super_cat_name)\n",
        "        super_cat_ids[super_cat_name] = nr_super_cats\n",
        "        super_cat_last_name = super_cat_name\n",
        "        nr_super_cats += 1\n",
        "\n",
        "print('Number of super categories:', nr_super_cats)\n",
        "print('Number of categories:', nr_cats)\n",
        "print('Number of annotations:', nr_annotations)\n",
        "print('Number of images:', nr_images)\n",
        "print('Number of scenes:', nr_scenes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TPUEaI4GKaV"
      },
      "source": [
        "In total, there are 4784 annotations for 60 classes. Waste were observed in 7 diverse enviroments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLilLTlXIgpA"
      },
      "outputs": [],
      "source": [
        "# This shows the scenes distribiution\n",
        "\n",
        "# Get scene cat names\n",
        "scene_cats = dataset['scene_categories']\n",
        "scene_name = []\n",
        "for scene_cat in scene_cats:\n",
        "    scene_name.append(scene_cat['name'])\n",
        "\n",
        "nr_scenes = len(scene_cats)\n",
        "scene_cat_histogram = np.zeros(nr_scenes,dtype=int)\n",
        "\n",
        "for scene_ann in dataset['scene_annotations']:\n",
        "    scene_ann_ids = scene_ann['background_ids']\n",
        "    for scene_ann_id in scene_ann_ids:\n",
        "        if scene_ann_id<len(scene_cats):\n",
        "            scene_cat_histogram[scene_ann_id]+=1\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({'scene_cats': scene_cats, 'nr_annotations': scene_cat_histogram})\n",
        "\n",
        "# Plot\n",
        "colors = ['white','black','gray', 'gold', 'red','green','lightskyblue']\n",
        "plt.pie(scene_cat_histogram, labels=scene_name, colors = colors,\n",
        "      shadow=False, startangle=-120)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7dhoP2oRPb8"
      },
      "outputs": [],
      "source": [
        "# This shows the number of annotations per category\n",
        "\n",
        "# Count annotations\n",
        "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
        "for ann in anns:\n",
        "    cat_histogram[ann['category_id']] += 1\n",
        "\n",
        "# Initialize the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(5,15))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
        "df = df.sort_values(by='Number of annotations', ascending=False)\n",
        "\n",
        "# Plot the histogram\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.set(style=\"whitegrid\")\n",
        "plot_1 = sns.barplot(x=\"Number of annotations\", y=\"Categories\", data=df,\n",
        "            label=\"Total\", color=\"b\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueLKAwznRTY7"
      },
      "outputs": [],
      "source": [
        "cat_ids_2_supercat_ids = {}\n",
        "for cat in categories:\n",
        "    cat_ids_2_supercat_ids[cat['id']] = super_cat_ids[cat['supercategory']]\n",
        "\n",
        "# Count annotations\n",
        "super_cat_histogram = np.zeros(nr_super_cats,dtype=int)\n",
        "for ann in anns:\n",
        "    cat_id = ann['category_id']\n",
        "    super_cat_histogram[cat_ids_2_supercat_ids[cat_id]] +=1\n",
        "\n",
        "# Initialize the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(5,10))\n",
        "\n",
        "# Convert to DataFrame\n",
        "d ={'Super categories': super_cat_names, 'Number of annotations': super_cat_histogram}\n",
        "df = pd.DataFrame(d)\n",
        "df = df.sort_values(by='Number of annotations', ascending=False)\n",
        "\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.set(style=\"whitegrid\")\n",
        "plot_1 = sns.barplot(x=\"Number of annotations\", y=\"Super categories\", data=df,\n",
        "                     label=\"Total\", color=\"b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-zazsjiGXEd"
      },
      "source": [
        "It is clear that there is a high class imbalance with some classes having fewer than 10 annotations, and some more than 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfCaQ5-2J2Qv"
      },
      "outputs": [],
      "source": [
        "# Visualize dataset graph\n",
        "\n",
        "#g = Digraph('G', filename='hello.gv')\n",
        "dot = Digraph('Dataset graph', filename='asd.gv')\n",
        "dot.attr(rankdir='LR', size='8,10')\n",
        "\n",
        "for cat_it in categories:\n",
        "    dot.node(cat_it['name'])\n",
        "    if cat_it['name']==cat_it['supercategory']:\n",
        "        dot.node(cat_it['supercategory'])\n",
        "    else:\n",
        "        dot.edge(cat_it['supercategory'], cat_it['name'])\n",
        "dot\n",
        "# Uncomment next line to print pdf\n",
        "#dot.view()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNuZ7QIvSGQy"
      },
      "source": [
        "# Detect-waste categories\n",
        "\n",
        "The [Detect-waste](https://detectwaste.netlify.app/) dataset is inspired by the waste segregation principles in Gdańsk, Poland, and it categorizes waste according to these principles, aiming to align with Polish recycling standards. The dataset proposes seven well-defined categories for sorting litter, which are reflective of the broader recycling categories recognized in Poland. These categories are:\n",
        "\n",
        "- **Bio**: Organic waste that can be composted.\n",
        "- **Glass**: All types of glass products.\n",
        "- **Metal and Plastic**: This category combines both metal and plastic waste, acknowledging the common collection of these materials in single recycling bins in some recycling schemes.\n",
        "- **Non-recyclable**: Waste that cannot be recycled and is typically destined for landfill or incineration.\n",
        "- **Other**: A category for waste that does not fit into the other categories or is of an ambiguous nature.\n",
        "- **Paper**: All types of paper and cardboard products.\n",
        "- **Unknown**: Items that cannot be easily classified into the above categories due to lack of visibility or information.\n",
        "\n",
        "These categories are based on the recycling rules in Gdańsk, Poland, and aim to provide a comprehensive framework for waste classification that can be used for automatic waste detection and sorting.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/majsylw/detect-waste-workshop/main/imgs/dw-cat.jpg\" alt=\"DW-cat\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX-FHvNpSF1N"
      },
      "outputs": [],
      "source": [
        "# Convert taco label to detect-waste labels based on polish recykling standards\n",
        "\n",
        "# Step 1. Manually assign categories from TACO to detectwaste\n",
        "\n",
        "def taco_to_detectwaste(label):\n",
        "    glass = [\"Glass bottle\", \"Broken glass\", \"Glass jar\"]\n",
        "    metals_and_plastics = [\"Aluminium foil\", \"Clear plastic bottle\",\"Other plastic bottle\",\n",
        "                           \"Plastic bottle cap\",\"Metal bottle cap\",\"Aerosol\",\"Drink can\",\n",
        "                           \"Food can\",\"Drink carton\",\"Disposable plastic cup\",\"Other plastic cup\",\n",
        "                           \"Plastic lid\",\"Metal lid\",\"Single-use carrier bag\",\"Polypropylene bag\",\n",
        "                           \"Plastic Film\",\"Six pack rings\",\"Spread tub\",\"Tupperware\",\n",
        "                           \"Disposable food container\",\"Other plastic container\",\n",
        "                           \"Plastic glooves\",\"Plastic utensils\",\"Pop tab\",\"Scrap metal\",\n",
        "                           \"Plastic straw\",\"Other plastic\", \"Plastic film\", \"Food Can\"]\n",
        "\n",
        "    non_recyclable = [\"Aluminium blister pack\",\"Carded blister pack\",\n",
        "                      \"Meal carton\",\"Pizza box\",\"Cigarette\",\"Paper cup\",\n",
        "                      \"Meal carton\",\"Foam cup\",\"Glass cup\",\"Wrapping paper\",\n",
        "                      \"Magazine paper\",\"Garbage bag\",\"Plastified paper bag\",\n",
        "                      \"Crisp packet\",\"Other plastic wrapper\",\"Foam food container\",\n",
        "                      \"Rope\",\"Shoe\",\"Squeezable tube\",\"Paper straw\",\"Styrofoam piece\",\n",
        "                      \"Rope & strings\", \"Tissues\"]\n",
        "\n",
        "    other = [\"Battery\"]\n",
        "    paper = [\"Corrugated carton\",\"Egg carton\",\"Toilet tube\",\"Other carton\", \"Normal paper\", \"Paper bag\"]\n",
        "    bio = [\"Food waste\"]\n",
        "    unknown = [\"Unlabeled litter\"]\n",
        "\n",
        "    if (label in glass):\n",
        "            label=\"glass\"\n",
        "    elif (label in metals_and_plastics):\n",
        "            label=\"metals_and_plastics\"\n",
        "    elif(label in non_recyclable):\n",
        "            label=\"non-recyclable\"\n",
        "    elif(label in other):\n",
        "            label=\"other\"\n",
        "    elif (label in paper):\n",
        "            label=\"paper\"\n",
        "    elif(label in bio):\n",
        "            label=\"bio\"\n",
        "    elif(label in unknown):\n",
        "            label=\"unknown\"\n",
        "    else:\n",
        "        print(label, \"is non-taco label\")\n",
        "        label = \"unknown\"\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vyfpz62SLdm"
      },
      "outputs": [],
      "source": [
        "# Step 2. Use function to convert annotations to desired form\n",
        "\n",
        "# convert all taco anns to detect-waste anns\n",
        "# let's change supercategory to detectwaste\n",
        "detectwaste_categories = dataset['categories']\n",
        "for ann in anns:\n",
        "    cat_id = ann['category_id']\n",
        "    cat_taco = categories[cat_id]['name']\n",
        "    detectwaste_categories[cat_id]['supercategory'] = taco_to_detectwaste(cat_taco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqLfJGxnM-FM"
      },
      "outputs": [],
      "source": [
        "# As there is no representation of \"Plastified paper bag\" in annotated data, change of this supercategory was done manually.\n",
        "\n",
        "print(detectwaste_categories[35])\n",
        "detectwaste_categories[35]['supercategory'] = taco_to_detectwaste(\"Plastified paper bag\")\n",
        "print(detectwaste_categories[35])\n",
        "#detectwaste_categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwSJLgjcNP-9"
      },
      "source": [
        "## Detect-waste evaluation\n",
        "It is extremely important to get to know what your dataset looks like. It might be helpfull during the evaluation of the system. Sometimes it happens that misclassification or other errors are caused by the imbalance if the dataset or some erroneous annotations. The first thing that we did was extracting from the dataset as much information as we could. Here you can see a few diagrams representing vital statistics.\n",
        "\n",
        "To prevent the negative effects of the data imbalance in our dataset, first we have to know how many images we have in each category. As you can see most of the trash found in our dataset is metals and plastics. Unfortunately, the second numerously represented category is unknown - the litter that has probably decomposed so much that it is hard to classify it. This makes our dataset highly imbalanced and will require special attention in future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKF2O_c6SVyu"
      },
      "outputs": [],
      "source": [
        "# Generate new ids for ploting histograms\n",
        "\n",
        "detectwaste_ids = {}\n",
        "detectwaste_cat_names = []\n",
        "cat_id = 0\n",
        "for cat in detectwaste_categories:\n",
        "    if cat['supercategory'] not in detectwaste_ids:\n",
        "        detectwaste_cat_names.append(cat['supercategory'])\n",
        "        detectwaste_ids[cat['supercategory']] = cat_id\n",
        "        cat_id += 1\n",
        "\n",
        "print(detectwaste_ids)\n",
        "print(detectwaste_cat_names)\n",
        "\n",
        "taco_to_detectwaste_ids = {}\n",
        "for i, cat in enumerate(detectwaste_categories):\n",
        "    taco_to_detectwaste_ids[cat['id']] = detectwaste_ids[cat['supercategory']]\n",
        "\n",
        "# print(taco_to_detectwaste_ids)\n",
        "\n",
        "colors_recykling = ['yellow', 'gray', 'gray', 'green', 'blue', 'brown', 'pink']\n",
        "\n",
        "anns_detectwaste = anns.copy()\n",
        "for i, ann in enumerate(anns):\n",
        "    #print(ann['category_id'])\n",
        "    anns_detectwaste[i]['category_id'] = taco_to_detectwaste_ids[ann['category_id']]\n",
        "    anns_detectwaste[i].pop('segmentation', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiROmBC7SiD9"
      },
      "outputs": [],
      "source": [
        "# Count annotations\n",
        "detectwaste_cat_histogram = np.zeros(len(detectwaste_cat_names),dtype=int)\n",
        "\n",
        "for ann in anns_detectwaste:\n",
        "    cat_id = ann['category_id']\n",
        "    detectwaste_cat_histogram[cat_id] +=1\n",
        "\n",
        "# Initialize the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(5,10))\n",
        "\n",
        "# Convert to DataFrame\n",
        "d ={'Super categories': detectwaste_cat_names, 'Number of annotations': detectwaste_cat_histogram}\n",
        "df = pd.DataFrame(d)\n",
        "df = df.sort_values(by='Number of annotations', ascending=False)\n",
        "\n",
        "sns.set_palette(sns.color_palette(colors_recykling))\n",
        "plot_1 = sns.barplot(x=\"Number of annotations\", y=\"Super categories\", data=df, label=\"Total\")\n",
        "plot_1.set_title('Annotations per detectwaste category',fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LbJms3ESiFt"
      },
      "source": [
        "## Visualize Annotated Images\n",
        "\n",
        "The script below shows how to filter images by either category or supercategory.\n",
        "\n",
        "Go ahead and try different (super)categories searches by changing the `category_name`. Note that small objects may be hard to see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw3iI3MQU0kc"
      },
      "outputs": [],
      "source": [
        "def extract_detectwaste_color(ann, taco_to_detectwaste_ids, colors_recykling):\n",
        "    color_id = taco_to_detectwaste_ids[ann['category_id']]\n",
        "    color = colors_recykling[color_id]\n",
        "    return color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFodZ79RShM7"
      },
      "outputs": [],
      "source": [
        "categories_to_show = ['Bottle', 'Shoe', 'Food waste']\n",
        "nr_img_2_display = 1\n",
        "pylab.rcParams['figure.figsize'] = (14,14)\n",
        "\n",
        "for category_name in categories_to_show: #  --- Insert the name of one of the categories or super-categories above\n",
        "\n",
        "    # Obtain Exif orientation tag code\n",
        "    for orientation in ExifTags.TAGS.keys():\n",
        "        if ExifTags.TAGS[orientation] == 'Orientation':\n",
        "            break\n",
        "\n",
        "    # Loads dataset as a coco object\n",
        "    coco = COCO(anns_file_path)\n",
        "\n",
        "    # Get image ids\n",
        "    imgIds = []\n",
        "    catIds = coco.getCatIds(catNms=[category_name])\n",
        "    if catIds:\n",
        "        # Get all images containing an instance of the chosen category\n",
        "        imgIds = coco.getImgIds(catIds=catIds)\n",
        "    else:\n",
        "        # Get all images containing an instance of the chosen super category\n",
        "        catIds = coco.getCatIds(supNms=[category_name])\n",
        "        for catId in catIds:\n",
        "            imgIds += (coco.getImgIds(catIds=catId))\n",
        "        imgIds = list(set(imgIds))\n",
        "\n",
        "    nr_images_found = len(imgIds)\n",
        "    print('Number of images found: ',nr_images_found)\n",
        "\n",
        "    # Select N random images\n",
        "    random.shuffle(imgIds)\n",
        "    imgs = coco.loadImgs(imgIds[0:min(nr_img_2_display,nr_images_found)])\n",
        "\n",
        "    for img in imgs:\n",
        "        image_path = TACO_DATA_FOLDER + img['file_name']\n",
        "        # Load image\n",
        "        I = Image.open(image_path)\n",
        "\n",
        "        # Load and process image metadata\n",
        "        if I._getexif():\n",
        "            exif = dict(I._getexif().items())\n",
        "            # Rotate portrait and upside down images if necessary\n",
        "            if orientation in exif:\n",
        "                if exif[orientation] == 3:\n",
        "                    I = I.rotate(180,expand=True)\n",
        "                if exif[orientation] == 6:\n",
        "                    I = I.rotate(270,expand=True)\n",
        "                if exif[orientation] == 8:\n",
        "                    I = I.rotate(90,expand=True)\n",
        "\n",
        "        # Show image\n",
        "        fig,ax = plt.subplots(1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(I)\n",
        "\n",
        "        # Load mask ids\n",
        "        annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n",
        "        anns_sel = coco.loadAnns(annIds)\n",
        "\n",
        "        # Show annotations\n",
        "        for ann in anns_sel:\n",
        "            color = extract_detectwaste_color(ann, taco_to_detectwaste_ids, colors_recykling)\n",
        "            for seg in ann['segmentation']:\n",
        "                poly = Polygon(np.array(seg).reshape((int(len(seg)/2), 2)))\n",
        "                p = PatchCollection([poly], facecolor=color, edgecolors=color,linewidths=0, alpha=0.4)\n",
        "                ax.add_collection(p)\n",
        "                p = PatchCollection([poly], facecolor='none', edgecolors=color, linewidths=2)\n",
        "                ax.add_collection(p)\n",
        "            [x, y, w, h] = ann['bbox']\n",
        "            rect = Rectangle((x,y),w,h,linewidth=2,edgecolor=color,\n",
        "                             facecolor='none', alpha=0.7, linestyle = '--')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngKot1cwVHVr"
      },
      "source": [
        "## Detect-waste statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8F6j1htWf5V"
      },
      "source": [
        "Information about the number of objects per image might be helpful during the detection process. We need to know if the majority of images contain only a single object or a few."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fK2YsijWisJ"
      },
      "outputs": [],
      "source": [
        "nr_annotation_per_image = []\n",
        "\n",
        "for img in dataset['images']:\n",
        "    annotations_per_image = []\n",
        "    for i in range(0, len(anns_detectwaste)):\n",
        "        if img['id'] == anns_detectwaste[i]['image_id']:\n",
        "            annotations_per_image.append(anns_detectwaste[i]['id'])\n",
        "    nr_annotation_per_image.append(len(annotations_per_image))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "ax = sns.distplot(nr_annotation_per_image,kde=False,bins=100, color='g')\n",
        "ax.set_yscale('log')\n",
        "ax.set(xlabel='Number of annotations per image', ylabel='Image Count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWbHSXYKXDxx"
      },
      "outputs": [],
      "source": [
        "def no_bbox_per_image(anns):\n",
        "    temo_im_ids = ([ann['image_id'] for ann in anns])\n",
        "    temp_im_ids = Counter(temo_im_ids)\n",
        "    list_of_duplicates = [temp_im_ids[i] for i,im_id in enumerate(temp_im_ids)]\n",
        "    list_of_duplicates = list(filter(lambda duplicate: duplicate != 0, list_of_duplicates))\n",
        "    return np.mean(list_of_duplicates)\n",
        "def no_bbox_summary(anns):\n",
        "    temo_im_ids = ([ann['image_id'] for ann in anns])\n",
        "    temp_im_ids = Counter(temo_im_ids)\n",
        "    list_of_duplicates = [temp_im_ids[i] for i,im_id in enumerate(temp_im_ids)]\n",
        "    list_of_duplicates = list(filter(lambda duplicate: duplicate != 0, list_of_duplicates))\n",
        "    print('Maximum bboxes: ',np.max(list_of_duplicates))\n",
        "    print('Mean number of bboxes: ',np.mean(list_of_duplicates))\n",
        "    print('Median number of bboxes: ',np.median(list_of_duplicates))\n",
        "\n",
        "\n",
        "print('all:',no_bbox_per_image(anns_detectwaste))\n",
        "for cat_nr, cat in enumerate(detectwaste_cat_names):\n",
        "    try:\n",
        "        temp_anns = [ann for ann in anns_detectwaste if(ann['category_id'] == cat_nr)]\n",
        "        print('\\n', cat)\n",
        "        no_bbox_summary(temp_anns)\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW1aBqJkXCbu"
      },
      "source": [
        "Some ML architectures require exact image size as an input, so it is worth to know what is the size of our data so that we could properly resize them. It also a good indicator of our general data quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq-rO4lmXgX-"
      },
      "outputs": [],
      "source": [
        "# Parsing image shapes (resolutions)\n",
        "widths = []\n",
        "heights = []\n",
        "shape_freqs = []\n",
        "img_shapes_keys = {}\n",
        "for img in dataset['images']:\n",
        "    key = str(img['width'])+'-'+str(img['height'])\n",
        "    if key in img_shapes_keys:\n",
        "        shape_id = img_shapes_keys[key]\n",
        "        shape_freqs[shape_id] += 1\n",
        "    else:\n",
        "        img_shapes_keys[key] = len(widths)\n",
        "        widths.append(img['width'])\n",
        "        heights.append(img['height'])\n",
        "        shape_freqs.append(1)\n",
        "\n",
        "d ={'Image width (px)': widths, 'Image height (px)': heights, '# images': shape_freqs}\n",
        "df = pd.DataFrame(d)\n",
        "cmap = sns.cubehelix_palette(dark=.1, light=.6, as_cmap=True)\n",
        "plot = sns.scatterplot(x=\"Image width (px)\", y=\"Image height (px)\", size='# images', hue=\"# images\", palette = cmap,data=df)\n",
        "plt.xlabel('Image width (px)', fontsize=15)\n",
        "plt.ylabel('Image height (px)', fontsize=15)\n",
        "plot = plot.set_title('Number of images per image shape',fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itakweGoXue7"
      },
      "source": [
        "Knowing where most of the objects occur in the image might be a useful information that can help us prevent some later errors. For instance, if our objects appear only in the center of an image, we should consider applying data augmentation methods to make the detector recognize objects in any other place in the image.\n",
        "\n",
        "Interestingly, in our case most of the objects are in the center. It might be due to the fact that at the same time most of the images contain only single object, typically in the center. In case of numerous objects, they usually are scattered through the whole image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzPJpoUSXvyT"
      },
      "outputs": [],
      "source": [
        "center_x = []\n",
        "center_y = []\n",
        "for i in range(0, len(anns_detectwaste)):\n",
        "    for j in range (0, len(dataset['images'])):\n",
        "        if dataset['images'][j]['id'] == anns_detectwaste[i]['image_id']:\n",
        "\n",
        "            center_x.append((anns_detectwaste[i]['bbox'][0]+anns_detectwaste[i]['bbox'][2]/2)/dataset['images'][j]['width'])\n",
        "            center_y.append((anns_detectwaste[i]['bbox'][1]+anns_detectwaste[i]['bbox'][3]/2)/dataset['images'][j]['height'])\n",
        "\n",
        "plt.figure(figsize=(30,15))\n",
        "plt.plot(center_x, center_y, 'bo')\n",
        "plt.title('Placement of central point of the bbox in the image', fontsize=30)\n",
        "plt.xlabel('Bbox x coordinate', fontsize=30)\n",
        "plt.ylabel('Bbox y coordinate', fontsize=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq1GdGgMX6Va"
      },
      "source": [
        "Also, the size of the bounding boxes is essential – we need to know if we will deal mostly with small or rather bigger objects. It is a well-known fact that even the state-of-the-art detectors do not work well with small objects.\n",
        "\n",
        "As our images vary in size, the bounding box size is also relative. It is worth to know how many annotations per different bounding box sizes we have in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvunN4W_YJM2"
      },
      "outputs": [],
      "source": [
        "bbox_widths = []\n",
        "bbox_heights = []\n",
        "obj_areas_sqrt = []\n",
        "obj_areas_sqrt_fraction = []\n",
        "bbox_aspect_ratio = []\n",
        "max_image_dim = 1024\n",
        "\n",
        "for ann in anns_detectwaste:\n",
        "\n",
        "    imgs = dataset['images']\n",
        "\n",
        "    resize_scale = max_image_dim/max(imgs[0]['width'], imgs[0]['height'])\n",
        "    # Uncomment this to work on original image size\n",
        "    # resize_scale = 1\n",
        "\n",
        "    bbox_widths.append(ann['bbox'][2]*resize_scale)\n",
        "    bbox_heights.append(ann['bbox'][3]*resize_scale)\n",
        "    obj_area = ann['bbox'][2]*ann['bbox'][3]*resize_scale**2 # ann['area']\n",
        "    obj_areas_sqrt.append(np.sqrt(obj_area))\n",
        "\n",
        "    img_area = imgs[0]['width']*imgs[0]['height']*resize_scale**2\n",
        "    obj_areas_sqrt_fraction.append(np.sqrt(obj_area/img_area))\n",
        "\n",
        "print('According to MS COCO Evaluation. This dataset has:')\n",
        "print(np.sum(np.array(obj_areas_sqrt)<32), 'small objects (area<32*32 px)')\n",
        "print(np.sum(np.array(obj_areas_sqrt)<64), 'medium objects (area<96*96 px)')\n",
        "print(np.sum(np.array(obj_areas_sqrt)<96), 'large objects (area>96*96 px)')\n",
        "\n",
        "# d ={'Bbox width (px)': bbox_widths, 'Bbox height (px)': bbox_heights, 'area': seg_areas}\n",
        "# df = pd.DataFrame(d)\n",
        "\n",
        "plt.figure(figsize=(30,15))\n",
        "ax = sns.distplot(obj_areas_sqrt_fraction,kde=False, bins=200, color='g')\n",
        "ax.set_yscale('log')\n",
        "plt.title('Number of annotations per relative bbox size', fontsize=30)\n",
        "plt.xlabel(r'Annotation relative size as $\\sqrt{ Bbox\\_area \\ /  \\ Image\\_area}$', fontsize=30)\n",
        "plt.ylabel('Number of annotations', fontsize=30)\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "ax = sns.distplot(np.maximum(np.array(bbox_widths),np.array(bbox_heights)),kde=False, bins=200, color='g')\n",
        "ax = ax.set(xlabel='Maximum bbox dimension', ylabel='Number of annotations')\n",
        "\n",
        "import colorsys\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
        "\n",
        "# Plotting bbox dims\n",
        "d ={'BBox width (px)': bbox_widths, 'BBox height (px)': bbox_heights}\n",
        "df = pd.DataFrame(d)\n",
        "cmap = sns.cubehelix_palette(dark=.1, light=.6, as_cmap=True)\n",
        "ax = sns.scatterplot(x=\"BBox width (px)\", y=\"BBox height (px)\", palette = cmap,data=df)\n",
        "\n",
        "print('Number of bboxes smaller than 1024:',np.sum(np.array(bbox_widths)<1024))\n",
        "print('Number of bboxes larger than 1024:',np.sum(np.array(bbox_widths)>1024))\n",
        "\n",
        "# anchors = [(32,32),(64,64),(128,128),(256,256),(512,512)]\n",
        "scales, ratios = np.meshgrid(np.array([16,32,64,128,256,512]), np.array([0.5,1,2]))\n",
        "scales = scales.flatten()\n",
        "ratios = ratios.flatten()\n",
        "# Enumerate heights and widths from scales and ratios\n",
        "anchor_heights = scales / np.sqrt(ratios)\n",
        "anchor_widths = scales * np.sqrt(ratios)\n",
        "\n",
        "IoUs = []\n",
        "for i in range(len(bbox_widths)):\n",
        "    bbox_area = bbox_widths[i]*bbox_heights[i]\n",
        "    IoU_max = 0.0\n",
        "    for j in range(len(anchor_heights)):\n",
        "        anchor_area = anchor_heights[j]*anchor_widths[j]\n",
        "        intersection_area = min(anchor_widths[j],bbox_widths[i])*min(anchor_heights[j], bbox_heights[i])\n",
        "        IoU = intersection_area / (bbox_area + anchor_area - intersection_area)\n",
        "        if IoU>0.5:\n",
        "            IoU_max = IoU\n",
        "    IoUs.append(IoU_max)\n",
        "\n",
        "print('Number of missing annotations', np.sum(np.array(IoUs)==0.0))\n",
        "\n",
        "# Plotting bbox dims\n",
        "d ={'BBox width (px)': bbox_widths, 'BBox height (px)': bbox_heights, 'IoU': IoUs}\n",
        "df = pd.DataFrame(d)\n",
        "cmap = sns.cubehelix_palette(dark=.1, light=.6, as_cmap=True)\n",
        "ax = sns.scatterplot(x=\"BBox width (px)\", y=\"BBox height (px)\", hue = 'IoU',data=df)\n",
        "plt.title('Bounding-boxes size', fontsize=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE3kuBHxVBAB"
      },
      "outputs": [],
      "source": [
        "def bbox_stats(anns, calc = 'mean', verbose = 1):\n",
        "    picsw = [pic['bbox'][2] for pic in anns]\n",
        "    picsh = [pic['bbox'][3] for pic in anns]\n",
        "    bbox_size = [w * h for w, h, in zip(picsw,picsh)]\n",
        "    if calc == 'mean':\n",
        "        return np.mean(bbox_size)\n",
        "    if calc == 'median':\n",
        "        return np.median(bbox_size)\n",
        "\n",
        "def area_stats(anns, calc = 'mean', verbose = 1):\n",
        "    picsw = [pic['area'] for pic in anns]\n",
        "    picsh = [pic['area'] for pic in anns]\n",
        "    area_size = [w * h for w, h, in zip(picsw,picsh)]\n",
        "    if calc == 'mean':\n",
        "        return np.mean(area_size)\n",
        "    if calc == 'median':\n",
        "        return np.median(area_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFEllAsiVLb6"
      },
      "outputs": [],
      "source": [
        "pylab.rcParams['figure.figsize'] = (12,6)\n",
        "\n",
        "mean_bbox = []\n",
        "median_bbox = []\n",
        "for cat_nr, cat in enumerate(detectwaste_cat_names):\n",
        "    temp_anns = [ann for ann in anns_detectwaste if(ann['category_id'] == cat_nr)]\n",
        "    mean_bbox.append(bbox_stats(temp_anns,))\n",
        "    median_bbox.append(bbox_stats(temp_anns,'median'))\n",
        "mean_bbox[-1] =0\n",
        "median_bbox[-1] =0\n",
        "\n",
        "# append stats for all for comparison\n",
        "temp_detectwaste_cat_names = detectwaste_cat_names.copy()\n",
        "temp_detectwaste_cat_names.append('all')\n",
        "mean_bbox.append(bbox_stats(anns_detectwaste))\n",
        "median_bbox.append(bbox_stats(anns_detectwaste,'median'))\n",
        "\n",
        "colors = []\n",
        "colors = colors_recykling.copy()\n",
        "colors.append('red')\n",
        "plt.bar(temp_detectwaste_cat_names,mean_bbox, color=colors)\n",
        "plt.title('Mean size of bbox')\n",
        "plt.show()\n",
        "\n",
        "plt.bar(temp_detectwaste_cat_names,median_bbox, color=colors)\n",
        "plt.title('Median size of bbox')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYWRrsPVNPi"
      },
      "outputs": [],
      "source": [
        "mean_area = []\n",
        "median_area = []\n",
        "for cat_nr, cat in enumerate(detectwaste_cat_names):\n",
        "    temp_anns = [ann for ann in anns_detectwaste if(ann['category_id'] == cat_nr)]\n",
        "    mean_area.append(area_stats(temp_anns,))\n",
        "    median_area.append(area_stats(temp_anns,'median'))\n",
        "\n",
        "mean_area[-1] = 0\n",
        "median_area[-1] = 0\n",
        "mean_area.append(area_stats(anns_detectwaste))\n",
        "median_area.append(area_stats(anns_detectwaste,'median'))\n",
        "print(temp_detectwaste_cat_names)\n",
        "\n",
        "plt.bar(temp_detectwaste_cat_names,mean_area, color=colors)\n",
        "plt.title('Mean size of area')\n",
        "plt.show()\n",
        "\n",
        "plt.bar(temp_detectwaste_cat_names,median_area, color=colors)\n",
        "plt.title('Median size of area')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x97DORa9V1ys"
      },
      "source": [
        "# Takeaways and Insights\n",
        "\n",
        "- The images are very large\n",
        "  - we should downsize the images prior to model training\n",
        "- The aspect ratio of most images show that they are probably taken on phone cameras in a vertical perspective. This could potential bias our inference if the edge devices are different.\n",
        "- The dataset has a few overrepresented classes (cigarettes, plastic film, unlabeled litter) and many categories with below 20 annotations.\n",
        "  - we will have to try out some data augmentation methods or weighted loss functions to make up for the imbalance,\n",
        "  - we have to take the imbalance into account when preparing dataset splits,\n",
        "- The data contains a lot of small objects:\n",
        "  - we will have to try architectures that are better with dealing with small objects,\n",
        "- Many images fall into the “unknown” category:\n",
        "  - we may try to cut images within bounding boxes and train a classifier. Perhaps then we will be able to classify those unknown objects to get their approximate categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCoAWUswZWAV"
      },
      "source": [
        "After above analysis we are ready start to prepare the data for training :)\n",
        "\n",
        "Can you think about more examinations which may be done here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSoc18Z5aOOm"
      },
      "source": [
        "# Additional excercise - spliting and cropping images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WT2foZMWHkq"
      },
      "outputs": [],
      "source": [
        "# Function for cropping images\n",
        "\n",
        "def crop(annotation_obj, fname, category_name, square, zoom, src_img, dst_img,  i):\n",
        "    # read information from 'annotations'\n",
        "    annotation_id = str(i) + str(annotation_obj['id'])\n",
        "    file_name = os.path.join(src_img, fname)\n",
        "\n",
        "    # prepare for cropping - USING THE BBOX's\n",
        "    # WIDTH AND HEIGHT HERE\n",
        "    x, y, width, height = annotation_obj['bbox']\n",
        "    img = cv2.imread(file_name)\n",
        "    if square:\n",
        "        if width > height:\n",
        "            x = x - (width-height)/2\n",
        "            height = width\n",
        "        else:\n",
        "            y = y - (-width+height)/2\n",
        "            width = height\n",
        "    width *= zoom\n",
        "    height *= zoom\n",
        "    crop_img = img[int(abs(y)): int(abs(y) + abs(height)),\n",
        "                   int(abs(x)): int(abs(x) + abs(width))]\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(\n",
        "            os.path.join(dst_img, category_name,\n",
        "                         annotation_id + '.jpg')), exist_ok=True)\n",
        "        cv2.imwrite(os.path.join(dst_img, category_name,\n",
        "                                 annotation_id + '.jpg'),\n",
        "                    crop_img)\n",
        "    except BaseException:\n",
        "        print(f\"ERROR: {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgrZTyy3azw4"
      },
      "outputs": [],
      "source": [
        "# path to source directory with images\n",
        "src_img = TACO_DATA_FOLDER\n",
        "# path to destination directory for images\n",
        "dst_img = HOME_FOLDER + 'images_square/'\n",
        "# cut images into square shape\n",
        "square = True\n",
        "# zoom out or in bounding box: useful for classification when used witg\n",
        "# detection algorithm that select not bbox coordinates\n",
        "# however can lower the scores if images are\n",
        "# crowded with many objects\n",
        "zoom = 1\n",
        "\n",
        "if not os.path.exists(dst_img):\n",
        "  os.mkdir(dst_img)\n",
        "\n",
        "\n",
        "waste_list = dataset['categories']\n",
        "\n",
        "print(waste_list)\n",
        "mapping_category = {}\n",
        "for item in waste_list:\n",
        "    mapping_category[item['id']] = item['supercategory']\n",
        "    if not os.path.exists(os.path.join(dst_img, item['supercategory'])):\n",
        "        os.mkdir(os.path.join(dst_img, item['supercategory']))\n",
        "\n",
        "# build a dictionary mapping the image id to the file name\n",
        "images = {}\n",
        "for img_obj in dataset['images']:\n",
        "    file_name = img_obj['file_name']\n",
        "    id = img_obj['id']\n",
        "    images[id] = file_name\n",
        "\n",
        "\n",
        "i = 0\n",
        "for annotation_obj in tqdm(dataset['annotations']):\n",
        "    category_name = mapping_category[annotation_obj['category_id']]\n",
        "    image_id = int(annotation_obj['image_id'])\n",
        "    crop(annotation_obj, images[image_id], category_name, square, zoom,\n",
        "         src_img, dst_img, i)\n",
        "    i += 1\n",
        "\n",
        "#dataset['annotations']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
